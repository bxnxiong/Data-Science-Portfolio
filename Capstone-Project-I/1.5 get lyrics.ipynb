{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics for billboard/MSD/extra songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def chunk_50(series): # requesting track info needs smaller size of requests\n",
    "    for i in xrange(0,len(series),50):\n",
    "        yield series[i:(i+50)]\n",
    "\n",
    "def get_spotify_names(spotifyIDs):\n",
    "    '''\n",
    "    in: [ids,...]\n",
    "    out: [(title,artist),...]\n",
    "    '''\n",
    "    names = []\n",
    "    \n",
    "    for chunked_spotifyIDs in chunk_50(spotifyIDs):\n",
    "        url_head = 'https://api.spotify.com/v1/tracks/?ids='\n",
    "        detail = ','.join(chunked_spotifyIDs)\n",
    "        url = url_head+detail\n",
    "        r = requests.get(url)\n",
    "        track_lists = r.json()['tracks']\n",
    "\n",
    "        for track in track_lists:\n",
    "            title = track['name'].lower()\n",
    "            artist = [i['name'].lower() for i in track['artists']] # only get first artist for now\n",
    "            names.append((title,artist))\n",
    "    return names\n",
    "\n",
    "def get_lyric(title,artist):\n",
    "    url_head = u'http://lyrics.wikia.com/wiki/'\n",
    "    url = url_head+urllib.quote(artist.encode('utf-8'))+':'+urllib.quote(title.encode('utf-8')) #encoding in utf-8 to avoid quote() throwing key error. eg.chanté moore\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "    lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "    lyric = re.sub('<br/>','.',lyric)\n",
    "    return lyric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_lyric_multiple_artists(ids,spotify_names,lyric_dict):\n",
    "    leftovers = []\n",
    "    for i,(title,artists) in enumerate(spotify_names):\n",
    "        if i % 100 == 0:\n",
    "            print i\n",
    "        uri = ids[i]\n",
    "        j= 0\n",
    "        while j <= len(artists)-1:\n",
    "            artist = artists[j]\n",
    "            lyric = get_lyric(title,artist)\n",
    "            if lyric != 'None' and uri not in lyric_dict: # if found lyric add that to the lyrics list\n",
    "                lyric_dict[uri] = lyric\n",
    "                break\n",
    "            j += 1\n",
    "        # if not found lyric for this title,artists\n",
    "        if lyric == 'None':\n",
    "            leftovers.append(uri)\n",
    "    return leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lyric2(title,artist): \n",
    "    # also look for redirect message\n",
    "    url_head = u'http://lyrics.wikia.com/wiki/'\n",
    "    url = url_head+urllib.quote(artist.encode('utf-8'))+':'+urllib.quote(title.encode('utf-8')) #encoding in utf-8 to avoid quote() throwing key error. eg.chanté moore\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "    \n",
    "    if lyricbox: # if url has lyricbox\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    elif soup.find('div',class_ = 'redirectMsg'): # if url is redirect message page\n",
    "        href = soup.find('div',class_ = 'redirectMsg').find('a')['href']\n",
    "        url = u'http://lyrics.wikia.com/'+href\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    else:\n",
    "        return 'None'\n",
    "def find_lyric_multiple_artists2(ids,spotify_names,lyric_dict):\n",
    "    leftovers = []\n",
    "    for i,(title,artists) in enumerate(spotify_names):\n",
    "        if i % 100 == 0:\n",
    "            print i\n",
    "        uri = ids[i]\n",
    "        j= 0\n",
    "        while j <= len(artists)-1:\n",
    "            artist = artists[j]\n",
    "            lyric = get_lyric2(title,artist)\n",
    "            if lyric != 'None' and uri not in lyric_dict: # if found lyric add that to the lyrics list\n",
    "                lyric_dict[uri] = lyric\n",
    "                break\n",
    "            j += 1\n",
    "        # if not found lyric for this title,artists\n",
    "        if lyric == 'None':\n",
    "            leftovers.append(uri)\n",
    "    return leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lyric3(title,artist): # href to url is changed, since some href has /wiki/... instead of wiki/...\n",
    "    url_head = u'http://lyrics.wikia.com/wiki/'\n",
    "    url = url_head+urllib.quote(artist.encode('utf-8'))+':'+urllib.quote(title.encode('utf-8')) #encoding in utf-8 to avoid quote() throwing key error. eg.chanté moore\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "    \n",
    "    if lyricbox: # if url has lyricbox\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    elif soup.find('div',class_ = 'redirectMsg'): # if url is redirect message page\n",
    "        href = soup.find('div',class_ = 'redirectMsg').find('a')['href']\n",
    "        url = u'http://lyrics.wikia.com'+href\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    else:\n",
    "        return 'None'\n",
    "\n",
    "def find_lyric_multiple_artists3(ids,spotify_names,lyric_dict):\n",
    "    leftovers = []\n",
    "    for i,(title,artists) in enumerate(spotify_names):\n",
    "        if i % 100 == 0:\n",
    "            print i\n",
    "        uri = ids[i]\n",
    "        j= 0\n",
    "        while j <= len(artists)-1:\n",
    "            artist = artists[j]\n",
    "            lyric = get_lyric3(title,artist)\n",
    "            if lyric != 'None' and uri not in lyric_dict: # if found lyric add that to the lyrics list\n",
    "                lyric_dict[uri] = lyric\n",
    "                break\n",
    "            j += 1\n",
    "        # if not found lyric for this title,artists\n",
    "        if lyric == 'None':\n",
    "            leftovers.append(uri)\n",
    "    return leftovers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just need to get spotify artist groups for both top10_spotify songs and MSD song ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('lyrics/All_names'):\n",
    "    audio_data = pd.read_pickle('MSD_audio_features')\n",
    "    spotifyIDs = audio_data.uri\n",
    "    audio_data = pd.read_pickle('Spotify_audio_features')\n",
    "    spotifyIDs = np.append(spotifyIDs,audio_data.uri)\n",
    "    ids = map(lambda x:x.split(':')[-1],spotifyIDs)\n",
    "    \n",
    "    spotify_names = get_spotify_names(ids) # get title,artists info for all songs(spotify+MSD)\n",
    "    spotifyNames = pd.DataFrame(spotify_names,columns = ['title','artists'])\n",
    "    spotifyNames['uri'] = ids\n",
    "    spotifyNames.to_pickle('lyrics/All_names')\n",
    "else:\n",
    "    spotifyNames = pd.read_pickle('lyrics/All_names')\n",
    "    ids = spotifyNames['uri']\n",
    "    spotify_names = zip(spotifyNames.title.values,spotifyNames.artists.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('lyrics/lyrics.pickle'):\n",
    "    lyrics = {}\n",
    "    leftovers = find_lyric_multiply_artists(ids,spotify_names,lyrics)\n",
    "    with open('lyrics/lyrics.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    leftover = pd.DataFrame(leftovers)\n",
    "    leftover.to_pickle('lyrics/Left_over_songs(no_lyric)')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics.pickle', 'r'))\n",
    "    leftovers = pd.read_pickle('lyrics/Left_over_songs(no_lyric)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covered 0.56 of songs\n"
     ]
    }
   ],
   "source": [
    "print 'Covered {:{prec}} of songs'.format((len(ids)-len(leftovers))/float(len(ids)),prec='.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are barely covering 55% of the songs!\n",
    "To get around that, I decided to get more songs from same artists that have produced hit songs on billboard. The benefit of this is that we can control the variation of different producing companies if we compare songs of same artist.\n",
    "\n",
    "But first I need to get all lyrics for billboard songs...done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Billboard song lyrics (covering 96% of 4264 songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = pd.read_pickle('Billboard_data')\n",
    "# get spotifyIDs for billboard songs\n",
    "audio_data = pd.read_pickle('Spotify_audio_features')\n",
    "spotifyIDs = map(lambda x:x.split(':')[-1],audio_data.uri.values)\n",
    "# get ids for those who don't have lyrics\n",
    "ids = [i for i in spotifyIDs if i not in lyrics]\n",
    "spotify_names = get_spotify_names(ids) # get title,artists info for spotifyIDs\n",
    "len(spotify_names) # 1111 songs to collect lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573 5102 573\n",
      "458 5217 458\n",
      "349 5326 349\n",
      "319 5356 319\n",
      "310 5365 310\n",
      "157 5518 157\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('lyrics/lyrics2.pickle'):\n",
    "    # billboard songs lyrics collection, filter 1: re.sub(' -.*','',x)\n",
    "    spotify_no_lyrics = pd.DataFrame(spotify_names,columns = ['title','artists'])\n",
    "    spotify_no_lyrics['uri'] = ids\n",
    "    spotify_no_lyrics.head(2)\n",
    "    spotify_no_lyrics['trimmed_title'] = map(lambda x:re.sub(' -.*','',x),spotify_no_lyrics.title)\n",
    "    names = zip(spotify_no_lyrics.trimmed_title.values,spotify_no_lyrics.artists)\n",
    "    ids   = spotify_no_lyrics.uri\n",
    "    leftovers = find_lyric_multiple_artists(ids,names,lyrics)\n",
    "    # save lyrics after filter 1\n",
    "    with open('lyrics/lyrics2.pickle', 'wb') as handle:\n",
    "            pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 2\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['spotifyID'] = leftovers\n",
    "    tmp.head()\n",
    "    tmp.to_pickle('lyrics/spotify_leftover_filter1')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics2.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/spotify_leftover_filter1')\n",
    "    no_lyrics = map(lambda x:x not in lyrics,tmp.spotifyID.values)\n",
    "    tmp = tmp[no_lyrics] # 573 songs\n",
    "    \n",
    "    \n",
    "# see how many we missed, how many we have lyrics for, and if the lyric is empty(if len(tmp)!=c)\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ids = uri.split(':')[-1]\n",
    "    if ids not in lyrics or (not lyrics[ids]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics3.pickle'):\n",
    "    # billboard songs lyrics collection, filter 2: use billboard data title names instead of spotify title names\n",
    "    spotify_leftover2 = pd.merge(tmp,top10,how='inner',on=['spotifyID']).ix[:,:5]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower(),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop(['date'],axis=1,inplace=True)\n",
    "    spotify_leftover2.drop_duplicates(subset=['spotifyID'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.spotifyID.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics3.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 2\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['spotifyID'] = leftovers\n",
    "    tmp.head()\n",
    "    tmp.to_pickle('lyrics/spotify_leftover_filter2')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics3.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/spotify_leftover_filter2')\n",
    "    spotify_leftover2 = pd.merge(tmp,top10,how='inner',on=['spotifyID']).ix[:,:5]\n",
    "    # 458 songs left\n",
    "    \n",
    "# see how many we missed, how many we have lyrics for, and if the lyric is empty(if len(tmp)!=c)\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ids = uri.split(':')[-1]\n",
    "    if ids not in lyrics or (not lyrics[ids]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "\n",
    "# billboard songs lyrics collection, filter 3: title.replace(' ','_'),artist.replace(' ','_')\n",
    "if not os.path.isfile('lyrics/lyrics4.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,top10,how='inner',on=['spotifyID']).ix[:,:5]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower().replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop(['date'],axis=1,inplace=True)\n",
    "    spotify_leftover2.drop_duplicates(subset=['spotifyID'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.spotifyID.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists2(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics4.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 3\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['spotifyID'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/spotify_leftover_filter3')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics4.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/spotify_leftover_filter3')\n",
    "    \n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ids = uri.split(':')[-1]\n",
    "    if ids not in lyrics or (not lyrics[ids]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics5.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,top10,how='inner',on=['spotifyID']).ix[:,:6]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower().replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop(['date'],axis=1,inplace=True)\n",
    "    spotify_leftover2.drop_duplicates(subset=['spotifyID'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.spotifyID.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics5.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['spotifyID'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/spotify_leftover_filter4')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics5.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/spotify_leftover_filter4')\n",
    "    \n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ids = uri.split(':')[-1]\n",
    "    if ids not in lyrics or (not lyrics[ids]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics6.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,top10,how='inner',on=['spotifyID']).ix[:,:6]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop(['date'],axis=1,inplace=True)\n",
    "    spotify_leftover2.drop_duplicates(subset=['spotifyID'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.spotifyID.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics6.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['spotifyID'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/spotify_leftover_filter5')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics6.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/spotify_leftover_filter5')\n",
    "    \n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ids = uri.split(':')[-1]\n",
    "    if ids not in lyrics or (not lyrics[ids]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics7.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,top10,how='inner',on=['spotifyID']).ix[:,:6]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop(['date'],axis=1,inplace=True)\n",
    "    spotify_leftover2.drop_duplicates(subset=['spotifyID'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.spotifyID.values\n",
    "    artists = map(lambda x:[x],spotify_leftover2['artist'].values)\n",
    "    names = zip(spotify_leftover2['title_y'].values,artists)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics7.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['spotifyID'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/spotify_leftover_filter6')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics7.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/spotify_leftover_filter6')\n",
    "\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ids = uri.split(':')[-1]\n",
    "    if ids not in lyrics or (not lyrics[ids]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covered: 0.96 Billboard songs\n"
     ]
    }
   ],
   "source": [
    "print 'Covered: {:{prec}} Billboard songs'.format((len(pd.read_pickle('Spotify_audio_features'))-float(c))/len(pd.read_pickle('Spotify_audio_features')),prec='.2') # decide to ignore these 157 songs since they are small percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics for MSD songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n"
     ]
    }
   ],
   "source": [
    "lyrics = pickle.load(open('lyrics/lyrics7.pickle', 'r'))\n",
    "# get spotifyIDs for MSD songs\n",
    "audio_data = pd.read_pickle('MSD_audio_features')\n",
    "spotifyIDs = map(lambda x:x.split(':')[-1],audio_data.uri.values)\n",
    "# get ids for those who don't have lyrics\n",
    "ids = [i for i in spotifyIDs if i not in lyrics]\n",
    "spotify_names = get_spotify_names(ids) # get title,artists info for spotifyIDs\n",
    "print len(spotify_names) # 2560 songs to collect lyrics\n",
    "MSD = pd.read_pickle('MSD_tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @overwrite get_lyric functions to avoid raising unicodeDecodeError\n",
    "# weird I can't actually find error when I run it outside the function:\n",
    "# À mon avis les bantous de la capitale\n",
    "\n",
    "def get_lyric(title,artist):\n",
    "    url_head = u'http://lyrics.wikia.com/wiki/'\n",
    "    try:\n",
    "        url = url_head+urllib.quote(artist.encode('utf-8'))+':'+urllib.quote(title.encode('utf-8')) #encoding in utf-8 to avoid quote() throwing key error. eg.chanté moore\n",
    "    except UnicodeDecodeError as e:\n",
    "        url = 'http://lyrics.wikia.com/wiki'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "    lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "    lyric = re.sub('<br/>','.',lyric)\n",
    "    return lyric\n",
    "\n",
    "def get_lyric2(title,artist):\n",
    "    # also look for redirect message\n",
    "    url_head = u'http://lyrics.wikia.com/wiki/'\n",
    "    try:\n",
    "        url = url_head+urllib.quote(artist.encode('utf-8'))+':'+urllib.quote(title.encode('utf-8')) #encoding in utf-8 to avoid quote() throwing key error. eg.chanté moore\n",
    "    except UnicodeDecodeError as e:\n",
    "        url = 'http://lyrics.wikia.com/wiki'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "\n",
    "    if lyricbox: # if url has lyricbox\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    elif soup.find('div',class_ = 'redirectMsg'): # if url is redirect message page\n",
    "        href = soup.find('div',class_ = 'redirectMsg').find('a')['href']\n",
    "        url = u'http://lyrics.wikia.com/'+href\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    else:\n",
    "        return 'None'\n",
    "    \n",
    "def get_lyric3(title,artist):\n",
    "    # href to url is changed, since some href has /wiki/... instead of wiki/...\n",
    "    url_head = u'http://lyrics.wikia.com/wiki/'\n",
    "    try:\n",
    "        url = url_head+urllib.quote(artist.encode('utf-8'))+':'+urllib.quote(title.encode('utf-8')) #encoding in utf-8 to avoid quote() throwing key error. eg.chanté moore\n",
    "    except UnicodeDecodeError as e:\n",
    "        print 't:_',title\n",
    "        print 'a:_',artist\n",
    "        url = 'http://lyrics.wikia.com/wiki'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "\n",
    "    if lyricbox: # if url has lyricbox\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    elif soup.find('div',class_ = 'redirectMsg'): # if url is redirect message page\n",
    "        href = soup.find('div',class_ = 'redirectMsg').find('a')['href']\n",
    "        url = u'http://lyrics.wikia.com'+href\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        lyricbox = soup.find('div',class_ = 'lyricbox')\n",
    "        lyric = re.sub('<.?div.*?>','',str(lyricbox))\n",
    "        lyric = re.sub('<br/>','.',lyric)\n",
    "        return lyric\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 result:\n",
      "2283 5789 2283\n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "Step 2 result:\n",
      "2237 5814 2258\n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "Step 3 result:\n",
      "2172 5879 2191\n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "t:_ À_mon_avis\n",
      "a:_ les bantous de la capitale\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "t:_ dang_pyar_da \n",
      "a:_ naseebo lal\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "t:_ ek_pardesi_di \n",
      "a:_ naseebo lal\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "t:_ lovesick_-_obsession_in_¾_time\n",
      "a:_ philippe sarde\n",
      "2100\n",
      "t:_ Èay!_que_pena\n",
      "a:_ los chunguitos\n",
      "Step 4 result:\n",
      "2168 5883 2187\n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "t:_ À_Mon_Avis\n",
      "a:_ les bantous de la capitale\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "t:_ Dang_Pyar_Da \n",
      "a:_ naseebo lal\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "t:_ Ek_Pardesi_Di \n",
      "a:_ naseebo lal\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "t:_ LOVESICK_-_Obsession_in_¾_Time\n",
      "a:_ philippe sarde\n",
      "2100\n",
      "t:_ Èay!_Que_Pena\n",
      "a:_ los chunguitos\n",
      "Step 5 result:\n",
      "2167 5884 2186\n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "t:_ À_Mon_Avis\n",
      "a:_ Les Bantous De La Capitale\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "t:_ Dang_Pyar_Da \n",
      "a:_ Naseebo Lal\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "t:_ Ek_Pardesi_Di \n",
      "a:_ Naseebo Lal\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "t:_ LOVESICK_-_Obsession_in_¾_Time\n",
      "a:_ Philippe Sarde\n",
      "2100\n",
      "t:_ Èay!_Que_Pena\n",
      "a:_ Los Chunguitos\n",
      "Step 6 result:\n",
      "2153 5898 2172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('lyrics/lyrics8.pickle'):\n",
    "    # MSD songs lyrics collection, filter 1: re.sub(' -.*','',x)\n",
    "    spotify_no_lyrics = pd.DataFrame(spotify_names,columns = ['title','artists'])\n",
    "    spotify_no_lyrics['uri'] = ids\n",
    "    spotify_no_lyrics.head(2)\n",
    "    spotify_no_lyrics['trimmed_title'] = map(lambda x:re.sub(' -.*','',x),spotify_no_lyrics.title)\n",
    "    \n",
    "    names = zip(spotify_no_lyrics.trimmed_title.values,spotify_no_lyrics.artists)\n",
    "    ids   = spotify_no_lyrics.uri\n",
    "    leftovers = find_lyric_multiple_artists(ids,names,lyrics)\n",
    "    \n",
    "    # save lyrics after filter 1\n",
    "    with open('lyrics/lyrics8.pickle', 'wb') as handle:\n",
    "            pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 2\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    tmp.head()\n",
    "    tmp.to_pickle('lyrics/MSD_leftover_filter1')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics8.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/MSD_leftover_filter1')\n",
    "\n",
    "print 'Step 1 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics9.pickle'):\n",
    "    # billboard songs lyrics collection, filter 2: use billboard data title names instead of spotify title names\n",
    "    spotify_leftover2 = pd.merge(tmp,Extra_tracks,how='inner',on=['uri'])\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower(),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics9.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 2\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    tmp.head()\n",
    "    tmp.to_pickle('lyrics/MSD_leftover_filter2')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics9.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/MSD_leftover_filter2')\n",
    "    \n",
    "print 'Step 2 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "# billboard songs lyrics collection, filter 3: title.replace(' ','_'),artist.replace(' ','_')\n",
    "if not os.path.isfile('lyrics/lyrics10.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,MSD,how='inner',on=['uri'])\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower().replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists2(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics10.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 3\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/MSD_leftover_filter3')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics10.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/MSD_leftover_filter3')\n",
    "\n",
    "        \n",
    "print 'Step 3 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics11.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,MSD,how='inner',on=['uri'])\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower().replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics11.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/MSD_leftover_filter4')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics11.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/MSD_leftover_filter4')\n",
    "\n",
    "print 'Step 4 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics12.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,MSD,how='inner',on=['uri']).ix[:,:6]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics12.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/MSD_leftover_filter5')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics12.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/MSD_leftover_filter5')\n",
    "\n",
    "print 'Step 5 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics13.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,MSD,how='inner',on=['uri']).ix[:,:6]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    artists = map(lambda x:[x],spotify_leftover2['artist'].values)\n",
    "    names = zip(spotify_leftover2['title_y'].values,artists)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics13.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/MSD_leftover_filter6')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics13.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/MSD_leftover_filter6')\n",
    "\n",
    "print 'Step 6 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covered 0.46 MSD songs.\n"
     ]
    }
   ],
   "source": [
    "uris = audio_data.uri\n",
    "c = 0\n",
    "for ID in map(lambda x:x.split(':')[-1],uris):\n",
    "    if ID in lyrics and lyrics[ID]!= 'None':\n",
    "        c += 1\n",
    "print 'Covered {:{prec}} MSD songs.'.format(float(c)/len(uris),prec='.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lyrics for same album/similar artist songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840\n"
     ]
    }
   ],
   "source": [
    "lyrics = pickle.load(open('lyrics/lyrics7.pickle', 'r'))\n",
    "# get spotifyIDs for songs\n",
    "audio_data = pd.read_pickle('Same_album_track_audio_features')\n",
    "spotifyIDs = map(lambda x:x.split(':')[-1],audio_data.uri.values)\n",
    "# get ids for those who don't have lyrics\n",
    "ids = [i for i in spotifyIDs if i not in lyrics]\n",
    "spotify_names = get_spotify_names(ids) # get title,artists info for spotifyIDs\n",
    "print len(spotify_names) # 3840 songs to collect lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('Extra_tracks'):\n",
    "    Extra_tracks = pd.DataFrame(spotify_names,columns=['title','artist'])\n",
    "    Extra_tracks['uri']=ids\n",
    "    Extra_tracks.to_pickle('Extra_tracks')\n",
    "else:\n",
    "    Extra_tracks = pd.read_pickle('Extra_tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 result:\n",
      "946 8412 946\n",
      "\n",
      "Step 2 result:\n",
      "945 8413 945\n",
      "\n",
      "Step 3 result:\n",
      "827 8531 827\n",
      "\n",
      "Step 4 result:\n",
      "799 8559 799\n",
      "\n",
      "Step 5 result:\n",
      "799 8559 799\n",
      "\n",
      "Step 6 result:\n",
      "799 8559 799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('lyrics/lyrics_same8.pickle'):\n",
    "    # songs lyrics collection, filter 1: re.sub(' -.*','',x)\n",
    "    spotify_no_lyrics = pd.DataFrame(spotify_names,columns = ['title','artists'])\n",
    "    spotify_no_lyrics['uri'] = ids\n",
    "    \n",
    "    spotify_no_lyrics['trimmed_title'] = map(lambda x:re.sub(' -.*','',x),spotify_no_lyrics.title)\n",
    "    \n",
    "    names = zip(spotify_no_lyrics.trimmed_title.values,spotify_no_lyrics.artists)\n",
    "    ids   = spotify_no_lyrics.uri\n",
    "    leftovers = find_lyric_multiple_artists(ids,names,lyrics)\n",
    "    \n",
    "    # save lyrics after filter 1\n",
    "    with open('lyrics/lyrics_same8.pickle', 'wb') as handle:\n",
    "            pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 2\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    tmp.head()\n",
    "    tmp.to_pickle('lyrics/same_leftover_filter1')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics_same8.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/same_leftover_filter1')\n",
    "\n",
    "print 'Step 1 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics_same9.pickle'):\n",
    "    # billboard songs lyrics collection, filter 2: use billboard data title names instead of spotify title names\n",
    "    spotify_leftover2 = pd.merge(tmp,Extra_tracks,how='inner',on=['uri'])\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower(),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics_same9.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 2\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    tmp.head()\n",
    "    tmp.to_pickle('lyrics/same_leftover_filter2')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics_same9.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/same_leftover_filter2')\n",
    "    \n",
    "print 'Step 2 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "# billboard songs lyrics collection, filter 3: title.replace(' ','_'),artist.replace(' ','_')\n",
    "if not os.path.isfile('lyrics/lyrics_same10.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,Extra_tracks,how='inner',on=['uri'])\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower().replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists2(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics_same10.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # collect leftover songs for filter 3\n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/same_leftover_filter3')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics_same10.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/same_leftover_filter3')\n",
    "\n",
    "        \n",
    "print 'Step 3 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics_same11.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,Extra_tracks,how='inner',on=['uri'])\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.lower().replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics_same11.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/same_leftover_filter4')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics_same11.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/same_leftover_filter4')\n",
    "\n",
    "print 'Step 4 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics_same12.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,Extra_tracks,how='inner',on=['uri']).ix[:,:6]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics_same12.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/same_leftover_filter5')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics_same12.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/same_leftover_filter5')\n",
    "\n",
    "print 'Step 5 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print\n",
    "\n",
    "if not os.path.isfile('lyrics/lyrics_same13.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,Extra_tracks,how='inner',on=['uri']).ix[:,:6]\n",
    "    spotify_leftover2['title_y'] = map(lambda x:x.replace(' ','_'),spotify_leftover2['title_y'].values)\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_multiple_artists3(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics_same13.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/same_leftover_filter6')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics_same13.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/same_leftover_filter6')\n",
    "\n",
    "print 'Step 6 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lyric_az(title,artist):\n",
    "    t = re.sub(r'\\W','',title.lower())\n",
    "    a = re.sub(r'\\W','',artist.lower())\n",
    "    url = 'http://www.azlyrics.com/lyrics/'+a+'/'+t+'.html'\n",
    "    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36'})\n",
    "    if r.status_code == 200:\n",
    "        lyricbox = re.findall(r'<!-- Usage of .*?>.*<!-- MxM banner -->',re.sub('\\n','',r.text))[0]\n",
    "        lyrics = re.sub('<br>','.',lyricbox)\n",
    "        lyrics = re.sub(r'<.*?>',' ',lyrics)\n",
    "        lyrics = re.sub('\\r','',lyrics)\n",
    "        lyrics = unicode(lyrics).encode('ascii','ignore')\n",
    "        return lyrics\n",
    "    return None\n",
    "\n",
    "def find_lyric_az(ids,spotify_names,lyric_dict):\n",
    "    leftovers = []\n",
    "    for i,(title,artists) in enumerate(spotify_names):\n",
    "        if i % 50 == 0:\n",
    "            print i\n",
    "        uri = ids[i]\n",
    "        j= 0\n",
    "        while j <= len(artists)-1:\n",
    "            artist = artists[j]\n",
    "            lyric = get_lyric_az(title,artist)\n",
    "            if lyric and uri not in lyric_dict: # if found lyric add that to the lyrics list\n",
    "                lyric_dict[uri] = lyric\n",
    "                break\n",
    "            j += 1\n",
    "        # if not found lyric for this title,artists\n",
    "        if not lyric:\n",
    "            leftovers.append(uri)\n",
    "        time.sleep(0.2)\n",
    "    return leftovers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "Step 7 result:\n",
      "746 8612 746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('lyrics/lyrics_final.pickle'):\n",
    "    # join table and remove duplicate rows\n",
    "    spotify_leftover2 = pd.merge(tmp,Extra_tracks,how='inner',on=['uri']).ix[:,:6]\n",
    "    spotify_leftover2.drop_duplicates(subset=['uri'],keep='first',inplace=True)\n",
    "\n",
    "    ids = spotify_leftover2.uri.values\n",
    "    names = zip(spotify_leftover2['title_y'].values,spotify_leftover2['artists'].values)\n",
    "    \n",
    "    leftovers = find_lyric_az(ids,names,lyrics)\n",
    "    \n",
    "    with open('lyrics/lyrics_final.pickle', 'wb') as handle:\n",
    "        pickle.dump(lyrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    leftover_names = get_spotify_names(leftovers)\n",
    "    tmp = pd.DataFrame(leftover_names,columns = ['title','artists'])\n",
    "    tmp['uri'] = leftovers\n",
    "    \n",
    "    tmp.to_pickle('lyrics/same_leftover_final')\n",
    "else:\n",
    "    lyrics = pickle.load(open('lyrics/lyrics_final.pickle', 'r'))\n",
    "    tmp = pd.read_pickle('lyrics/same_leftover_final')\n",
    "\n",
    "print 'Step 7 result:'\n",
    "c = 0\n",
    "for uri in audio_data['uri']:\n",
    "    ID = uri.split(':')[-1]\n",
    "    if ID not in lyrics or (not lyrics[ID]):\n",
    "        c += 1\n",
    "print len(tmp),len(lyrics),c\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
